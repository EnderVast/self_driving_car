# -*- coding: utf-8 -*-
"""self_driving_car.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nSN4BFL3CfR34pxT-srsAR6erUCDokc0
"""

!git clone https://github.com/EnderVast/self_driving_car.git
!ls self_driving_car
!pip3 install imgaug

import os
import numpy as np
import matplotlib.pyplot as plt
import keras
import matplotlib.image as mpimg
from imgaug import augmenters as iaa
from keras.models import Sequential
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
import cv2
from sklearn.utils import shuffle
import pandas as pd
import random
import ntpath
import csv

#Import training data CSV
datadir = 'self_driving_car/data'
columns = ('image', 'steering')
data = pd.read_csv(os.path.join(datadir, 'img_dir.csv'), header = None, names = columns)
data.steering = data.steering.astype('float')
print(type(data.steering[0]))
#Reduce clutter: remove file paths (Keep image name)
def path_leaf(path):
  head, tail = ntpath.split(path)
  return tail
data['image'] = data['image'].apply(path_leaf)
#Import all image paths into a list
img_path = os.listdir(os.path.join(datadir, 'recorded_images'))
#Convert degrees to radians (final steering value is angle from center(0))
pi = 22/7
for i in range(len(data['steering']) - 1):
  if data.at[i, 'steering'] > 96.0:   #right
    corrected_angle = float((data.at[i, 'steering'] - 90) * (pi/180))
    data.at[i, 'steering'] = round(corrected_angle, 3)
  elif data.at[i, 'steering'] < 84.0:   #left
    corrected_angle = float((pi/2) - (data.at[i, 'steering'] * (pi/180)))
    data.at[i, 'steering'] = -round(corrected_angle, 3)
  else:
    data.at[i, 'steering'] = 0.000
data.head()

#Create new dataframe but without deleted images
img_path_all = os.listdir(datadir + "/recorded_images")
to_keep = []
for i in range(len(data['image']) - 1):
  to_keep_row = []
  for j in img_path_all:
    if data['image'][i] == j:
      to_keep_row.append(data.at[i, 'image'])
      to_keep_row.append(data.at[i, 'steering'])
      to_keep.append(to_keep_row)
data = pd.DataFrame(to_keep, columns = ['image_path', 'steering_angle'])

"""bins stores bin edges"""

#Visualize data
num_bins = 25
hist, bins = np.histogram(data['steering_angle'], bins = num_bins)
center = (bins[:-1] + bins[1:]) * 0.5
print(center)
plt.bar(center, hist, width = 0.05)
print("Total data: ", len(data['image_path']))

data = pd.DataFrame(to_keep, columns = ['image_path', 'steering_angle'])

remove_list = []
max_samples_per_bin = 500
for j in range(num_bins):
  list_ = []
  for i in range(len(data['steering_angle'])):
    if data['steering_angle'][i] >= bins[j] and data['steering_angle'][i] <= bins[j+1]:
      list_.append(i)
  list_ = shuffle(list_)
  list_ = list_[max_samples_per_bin:]  #list to remove
  remove_list.extend(list_)
  
print(remove_list)
print('removed:', len(remove_list))
data.drop(data.index[remove_list], inplace = True)
print('remaining:', len(data))

hist, _ = np.histogram(data['steering_angle'], (num_bins))
plt.bar(center, hist, width = 0.05)   #range, histogram vals,width
plt.plot((np.min(data['steering_angle']), np.max(data['steering_angle'])), (max_samples_per_bin, max_samples_per_bin))

#Prepare training data for splitting into test and validation sets
image_paths = []
steering = []
for i in range(len(data)):
  indexed_data = data.iloc[i]
  image_paths.append(datadir + "/recorded_images/" + indexed_data[0])
  steering.append(indexed_data[1])
#Split training data
X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steering, test_size = 0.2, random_state = 6)
print('Training Samples: {} \n Valid Samples: {}'.format(len(X_train), len(X_valid)))
#Visualize prepared data
fig, axes = plt.subplots(1, 2, figsize = (12, 4))   #1 row 2 col of graphs, 12 by 4 graph
axes[0].hist(y_train, bins = num_bins, width = 0.05, color = 'blue')
axes[0].set_title('Training set')
axes[1].hist(y_valid, bins = num_bins, width = 0.05, color = 'r')
axes[1].set_title('Validation set')

#Augmentation
def zoom(image):
  zoom = iaa.Affine(scale = (1, 1.3))   #Zoom from 1(no zoom) to 1.3x
  image = zoom.augment_image(image)
  return image

def pan(image):
  pan = iaa.Affine(translate_percent = {'x': (-0.1, 0.1), 'y': (-0.1, 0.1)})   #pan max 10% left or right
  image = pan.augment_image(image)   #apply pan transformation
  return image

def img_random_brightness(image):
  brightness = iaa.Multiply((0.2, 1.2))
  image = brightness.augment_image(image)
  return image

def img_random_flip(image, steering_angle):    #steering angle is to invert the angle from data because turning in another direction
  image = cv2.flip(image, 1)    #second argument type of flip, 0: vertical, 1: horizontal, -1: combo
  steering_angle = -steering_angle
  return image, steering_angle

def random_augment(image, steering_angle):
  image = mpimg.imread(image)     #run 50% of the time
  if np.random.rand() < 0.5:
    image = pan(image)
  if np.random.rand() < 0.5:
    image = zoom(image)
  if np.random.rand() < 0.5:
    image = img_random_brightness(image)
  if np.random.rand() < 0.5:
    image, steering_angle = img_random_flip(image, steering_angle)
  return image, steering_angle

#Visualize augmentation
fig, axs = plt.subplots(5, 2, figsize = (10, 10))
fig.tight_layout()

for i in range(5):
  randnum = random.randint(0, len(image_paths) - 1)
  random_image = image_paths[randnum]
  random_steering = steering[randnum]
  
  ori_img = mpimg.imread(random_image)
  augmented_image, steering_angle = random_augment(random_image, random_steering)
  
  axs[i][0].imshow(ori_img)
  axs[i][0].set_title('ori img')
  
  axs[i][1].imshow(augmented_image)
  axs[i][1].set_title('aug img')

#To comply with Nvidia's model's requirements
def img_preprocess(img):
  img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
  img = cv2.GaussianBlur(img, (3, 3), 0)
  img = cv2.resize(img, (200, 66))
  img = img/255
  return img

#Show difference between ori and preprocessed image
image = image_paths[200]
original_image = mpimg.imread(image)
preprocessed_image = img_preprocess(original_image)
fig, axs = plt.subplots(1, 2, figsize = (15,10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title('Original image')
axs[1].imshow(preprocessed_image)
axs[1].set_title('Preprocessed image')

def batch_generator(image_paths, steering_ang, batch_size, istraining):
  while True:
    batch_img = []
    batch_steering = []
    for i in range(batch_size):
      random_index = random.randint(0, len(image_paths) - 1)
      
      if istraining == True:
        im, steering = random_augment(image_paths[random_index], steering_ang[random_index])
      else:
        im = mpimg.imread(image_paths[random_index])
        steering = steering_ang[random_index]
        
      im = img_preprocess(im)
      batch_img.append(im)
      batch_steering.append(steering)
      
    yield(np.asarray(batch_img), np.asarray(batch_steering))

x_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, True))
x_val_gen, y_value_gen = next(batch_generator(X_valid, y_valid, 1, False))

fig, axs = plt.subplots(1, 2, figsize = (15, 10))
fig.tight_layout()
axs[0].imshow(x_train_gen[0])
axs[0].set_title('Training Img')

axs[1].imshow(x_val_gen[0])
axs[1].set_title('Validation Img')

print(x_train_gen[0].ndim)

def nvidia_model():
  model = Sequential()
  model.add(Conv2D(24, 5, 5, subsample = (2, 2), input_shape = (66, 200, 3), activation = 'elu'))
  model.add(Conv2D(36, 5, 5, subsample = (2, 2), activation = 'elu'))
  model.add(Conv2D(48, 5, 5, subsample = (2, 2), activation = 'elu'))
  model.add(Conv2D(64, 3, 3, activation = 'elu'))
  model.add(Conv2D(64, 3, 3, activation = 'elu'))
  
  model.add(Flatten())
  model.add(Dense(100, activation = 'elu'))
  
  model.add(Dense(50, activation = 'elu'))
  model.add(Dense(10, activation = 'elu'))
  model.add(Dense(1))
  
  optimizer = Adam(lr = 0.0001)
  model.compile(loss = 'mse' , optimizer = optimizer)
  return model

model = nvidia_model()
print(model.summary())

history = model.fit_generator(batch_generator(X_train, y_train, 100, True), steps_per_epoch = 300, epochs = 10, validation_data = batch_generator(X_valid, y_valid, 100, False), validation_steps = 200, verbose = 1, shuffle = 1)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('loss')
plt.xlabel('Epoch')

model.save('model.h5')

from google.colab import files
files.download('model.h5')

